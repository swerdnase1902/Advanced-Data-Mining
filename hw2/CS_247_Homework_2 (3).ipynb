{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 for CS 247 : Advanced Data Mining Learning\n",
    "\n",
    "### Due: 11:59 pm 04/14\n",
    "\n",
    "\n",
    "__Name__: Chiao Lu\n",
    "\n",
    "__UID__: 204848946"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1:  Logistic Regression\n",
    "\n",
    "1. Write down the matrix form operation for gradient vector and Hessian matrix for logistic regression, according to Slide 35 and Slide 36;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Your answer here:\n",
    "gradient:\n",
    "<br>\n",
    "Let $$X\\in \\mathbb{R}^{N\\times (p+1)}$$, $$Y\\in \\mathbb{R}^{N}$$, and $$p(\\beta) \\in \\mathbb{R}^{N}$$ where $$p(\\beta)=\\sigma (\\beta^{T} X)$$\n",
    "<br>\n",
    "$$\\frac{\\partial \\log L(\\beta)}{\\partial \\beta}=X^{T} \\times (Y-p(\\beta))$$\n",
    "<br>\n",
    "<br>\n",
    "Hessian:\n",
    "<br>\n",
    "Let $$\\mathrm{diag} (A)$$ be the same dimension as $$A$$ such that $$(\\mathrm{diag} (A))_{ij}=\\left\\{\\begin{matrix}A_{ij}&i=j\\\\0&i\\neq j\\\\\\end{matrix}\\right.$$ \n",
    "<br>\n",
    "Then \n",
    "$$\\frac{\\partial^{2} \\log L(\\beta)}{\\partial \\beta^{2}}=-X^{T} \\times \\mathrm{diag(p(\\beta))} \\times \\mathrm{diag(1-p(\\beta))} \\times X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. When implementing ML algorithms, it is important to paralize numerical computation and avoid for-loop computation. For optimizing Logistic Regression, we've alreay implemented the ***for-loop*** version of ***Gradient Ascent*** algorithm. Please implement ***matrix*** version of ***Gradient Ascent***, implement ***My_Logistic_Regression*** for training and prediction. Finally, Compare the accuracy and running time of the sklearn Logistic Regression with these two optimization.\n",
    "\n",
    "3. Implement ***Newton's Method*** (matrix version), compare its accuracy, running time with ***Gradient Ascent***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001B[0;36m  File \u001B[0;32m\"<ipython-input-1-a3f43d1dcd12>\"\u001B[0;36m, line \u001B[0;32m41\u001B[0m\n\u001B[0;31m    gradient =\u001B[0m\n\u001B[0m               ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ],
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-a3f43d1dcd12>, line 41)",
     "output_type": "error"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report \n",
    "import numpy as np\n",
    "import time\n",
    "   \n",
    "    \n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z)) \n",
    "# ==================\n",
    "#  Three different Optimizers\n",
    "# ==================\n",
    "\n",
    "\n",
    "def forloop_GA_optimizer(W, X, y):\n",
    "    \"\"\"\n",
    "    Gradient Ascent (GA) optimizer implemented with for loop.\n",
    "\n",
    "    Args:\n",
    "        W - Parameters\n",
    "        X - Features of training batch/instance\n",
    "        y - Label(s) of training batch/instance\n",
    "    Return:\n",
    "        W_new - Updated W\n",
    "    \"\"\"\n",
    "    learning_rate = 1e-2\n",
    "    n_data, n_features = X.shape[0], X.shape[1] #total number of attributes\n",
    "    gradient = np.zeros(n_features)\n",
    "    for j in range(n_features):\n",
    "        for i in range(n_data):\n",
    "            gradient[j] += X[i][j] * (y[i] - sigmoid(np.dot(np.transpose(W), X[i])))\n",
    "    W += learning_rate * gradient\n",
    "\n",
    "\n",
    "\n",
    "def matrix_GD_optimizer(W, X, y):\n",
    "    \"\"\"\n",
    "    Gradient Ascent (GA) optimizer implemented with matrix operations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ========= TODO start ==========\n",
    "    learning_rate = 1e-2\n",
    "    gradient = np.transpose(X) @ (y - sigmoid(W@X))\n",
    "    W += learning_rate*gradient\n",
    "    # ========= TODO end ============\n",
    "\n",
    "\n",
    "\n",
    "def matrix_Newton_optimizer(W, X, y):\n",
    "    \"\"\"\n",
    "    Newton's method optimizer implemented with matrix operations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ========= TODO start ==========\n",
    "    hessian = -np.transpose(X)@np.diag(sigmoid(W@X))@np.diag(1 - sigmoid(W@X))@X\n",
    "    hessian_inv = np.linalg.inv(hessian)\n",
    "    gradient = np.transpose(X) @ (y - sigmoid(W@X))\n",
    "    W -= hessian_inv@gradient\n",
    "    # ========= TODO end ============\n",
    "    \n",
    "class My_Logistic_Regression():\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "        n_features : int, feature dimension\n",
    "        optimizer  : function, one optimizer that takes input the model weights\n",
    "                     and training data to perform one iteration of optimization.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, optimizer=matrix_GD_optimizer):\n",
    "        self.W = np.random.rand(n_features)\n",
    "        self.optimizer = optimizer\n",
    "    def fit(self, X, y):\n",
    "        n_epoch = 10000\n",
    "        for epoch in range(n_epoch):\n",
    "            self.optimizer(self.W, X, y)\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "n_data, n_features = X.shape[0], X.shape[1]\n",
    "ids  = np.arange(n_data)\n",
    "np.random.shuffle(ids)\n",
    "train_ids, test_ids = ids[: n_data // 2], ids[n_data // 2: ]\n",
    "\n",
    "train_X, train_y = X[train_ids], y[train_ids]\n",
    "test_X,  test_y  = X[test_ids],  y[test_ids]\n",
    "clf = LogisticRegression()\n",
    "start_time = time.time()\n",
    "clf.fit(train_X, train_y)\n",
    "end_time = time.time()\n",
    "print('Training time: %d s' % (end_time - start_time))\n",
    "pred_y = clf.predict(test_X)\n",
    "print(classification_report(pred_y, test_y))\n",
    "\n",
    "\n",
    "for optimizer in [forloop_GA_optimizer, matrix_GD_optimizer]:\n",
    "    my_clf = My_Logistic_Regression(n_features, optimizer)\n",
    "    start_time = time.time()\n",
    "    my_clf.fit(train_X, train_y)\n",
    "    end_time = time.time()\n",
    "    print('Training time: %d s' % (end_time - start_time))\n",
    "    pred_y = my_clf.predict(test_X)\n",
    "    print(classification_report(pred_y, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 Poisson Regressions\n",
    "\n",
    "Poisson regression is another example of generalized linear model, which assumes the label $y$ has a Poisson distribution.\n",
    "\n",
    "1. Prove Poisson distribution belongs to exponential family;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Your answer here:\n",
    "\n",
    "$$P(Y=y)=\\frac{\\lambda^{y}\\exp(-\\lambda)}{y!}$$\n",
    "$$=\\frac{\\exp(\\log{\\lambda^{y}})\\exp(-\\lambda)}{y!}=\\frac{\\exp(y \\log{\\lambda})\\exp(-\\lambda)}{y!}=\\frac{\\exp(y \\log(\\lambda) - \\lambda)}{y!}$$\n",
    "<br>\n",
    "Now, let $$b(y)=\\frac{1}{y!}$$, $$\\eta=\\log(\\lambda)$$, $$T(y)=y$$, and $$a(\\eta)=\\exp(\\eta)$$.\n",
    "<br>\n",
    "With the above substitution, we get $$b(y)\\exp( \\eta^{T} T(y)-a(\\eta))$$. This tells us that Poission belongs to exponential family."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Follow the recipe of GLM, write down Poisson regression model. Then write down how to train the model given a dataset $D = \\{x_i, y_i\\}^n_{i=1}$, following MLE (Maximum Likelihood Estimation) paradigm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Your answer here:\n",
    "OK.\n",
    "<br>\n",
    "Substitue $$\\eta=\\log(\\lambda)=\\beta^{T} x$$ in the above $$P(Y=y)$$ and we get $$P(Y=y)=\\frac{\\exp(y \\beta^{T}x - \\exp(\\beta^{T} x))}{y!}$$\n",
    "<br>\n",
    "The joint probablity of observing the training data is then<br>\n",
    "$$P(y_{1}, y_{2},...y_{n}|x_{1}, x{2},...,X_{n}, \\beta)=\\prod_{i=1}^{n}\\frac{\\exp(y_{i} \\beta^{T}x_{i} - \\exp(\\beta^{T} x_{i}))}{y_{i}!}$$\n",
    "<br>\n",
    "The log-likelihood is then $$\\mathrm{LL}=\\sum_{i=1}^{n}y_{i} \\beta^{T}x_{i} - \\exp(\\beta^{T} x_{i})-\\log(y_{i}!)$$\n",
    "<br>\n",
    "We wish to find $${\\arg \\max}_{\\beta}\\mathrm{LL}(\\beta)$$.\n",
    "To do MLE we take gradient of $$\\mathrm{LL}$$ with respect to $$\\beta$$.\n",
    "<br>\n",
    "We have $$\\frac{\\partial \\mathrm{LL}(\\beta)}{\\partial \\beta} = \\sum_{i=1}^{n}y_{i} x_{i} - x_{i} \\exp(\\beta^{T} x_{i}) = \\sum_{i=1}^{n} x_{i} (y_{i}-\\exp(\\beta^{T} x_{i}))$$\n",
    "<br>\n",
    "With the gradient, we can now perform gradient descent!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}