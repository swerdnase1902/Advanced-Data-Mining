{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 for CS 247 : Advanced Data Mining Learning\n",
    "\n",
    "### Due: 11:59 pm 04/07\n",
    "\n",
    "\n",
    "__Name__: Chiao Lu\n",
    "\n",
    "__UID__: 204848946"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 Multinomial Naive Bayes Optimization\n",
    "\n",
    "For multinomial naive Bayes model, prove the MLE estimator Î² for what is stated in Slide 21."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to maximize $$\\sum_{d}\\sum_{n}{x_{dn}\\log{(\\beta_{y_dn})}}=\\sum_{d}\\sum_{n}\\sum_{j}{x_{dn}\\mathbb{1}\\left(y_d=j\\right)\\log{\\left(\\beta_{jn}\\right)}}$$.\n",
    "<br>\n",
    "Let $$F\\left(\\beta_{jn}\\right)$$ be the above expression.\n",
    "<br>\n",
    "Now, $$\\frac{\\partial F}{\\partial\\beta_{jn}}=\\sum_{d}{x_{dn}\\mathbb{1}\\left(y_d=j\\right)\\times\\frac{1}{\\beta_{jn}}}$$\n",
    "<br>\n",
    "With $$\\frac{\\partial F}{\\partial\\beta_{jn}}=\\sum_{d}{x_{dn}\\mathbb{1}\\left(y_d=j\\right)\\times\\frac{1}{\\beta_{jn}}}=\\lambda$$ solve for $$\\lambda$$ using the fact that $$\\sum_{n}{\\beta_{jn}}=1$$, we get $$\\lambda=\\sum_{d}{\\sum_{n}{x_{dn}1(y_{d}=j)}}$$\n",
    "<br>\n",
    "Substitute the $$\\lambda$$ into above and get $$\\beta_{jn}=\\frac{\\sum_{d}{x_{dn}1(y_{d}=j)}}{\\sum_{d}{\\sum_{n}{x_{dn}1(y_{d}=j)}}}$$, which is equivalent to the expression given in the lecture slides.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2:  Multinomial Naive Bayes Implementation\n",
    "\n",
    "\n",
    "In this problem, we'd like you to implement naive bayes, and apply it on a real-world sentiment classification dataset. \n",
    "\n",
    "1. We've provided a general framework \"My_MultinomialNB\" below, please fill ''TODO'' slots. More specifically, you should implement the ***fit***, ***predict_proba_without_log*** and ***predict_log_proba_with_log*** functions. \n",
    "\n",
    "    For function ***fit***, given a training dataset with feature ***X*** and label ***y***, we'd like you to calculate ***beta*** and ***pi*** with a smoothing parameter ***alpha*** (laplace smoothing). \n",
    "\n",
    "    For ***predict_proba_without_log***, given a test dataset with feature X, we'd like you to calculate the predicted probability of each data point using what we've learned in class.\n",
    "\n",
    "    For ***predict_log_proba_with_log***, given a test dataset with feature ***X***, we'd like you to calculate the log probability of each data point, using ***log_beta*** and ***log_pi***. With this function, we can also get probability using ***predict_proba_with_log***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class My_MultinomialNB():\n",
    "    \"\"\"\n",
    "    Multinomial Naive Bayes (MultinomialNB)\n",
    "    ==========  \n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : float, optional (default=1.0)\n",
    "        Additive (Laplace/Lidstone) smoothing parameter\n",
    "        (0 for no smoothing).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=1):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.class_indicator = {}\n",
    "        for i, c in enumerate(np.unique(y)):\n",
    "            self.class_indicator[c] = i\n",
    "        self.n_class = len(self.class_indicator)\n",
    "        self.n_feats = np.shape(X)[1]\n",
    "        \n",
    "        self.beta    = np.zeros((self.n_class, self.n_feats))\n",
    "        self.pi      = np.zeros((self.n_class))\n",
    "        '''\n",
    "            TODO: Calculate self.beta and self.pi\n",
    "        '''\n",
    "        for row in range(self.beta.shape[0]):\n",
    "            X_correct_class = X[y==row]\n",
    "            numerator = np.sum(X_correct_class, axis=0)\n",
    "            denominator = np.sum(numerator)\n",
    "            numerator += self.alpha\n",
    "            denominator += self.alpha*self.n_feats\n",
    "            self.beta[row] = numerator/denominator\n",
    "        self.pi = np.unique(y, return_counts=True)[1]/len(y)\n",
    "        self.log_beta = np.log(self.beta)\n",
    "        self.log_pi   = np.log(self.pi)\n",
    "        \n",
    "    def predict(self, X, with_log = True):\n",
    "        if with_log:\n",
    "            probability = self.predict_proba_with_log(X)\n",
    "        else:\n",
    "            probability = self.predict_proba_without_log(X)\n",
    "        return np.argmax(probability, axis=1)    \n",
    "    \n",
    "    def predict_proba_without_log(self, X):\n",
    "        prob = np.zeros((len(X), self.n_class))\n",
    "        '''\n",
    "            TODO: Calculate probability of which class each data belongs to, using self.beta and self.pi\n",
    "        '''\n",
    "        for sample_idx in range(len(X)):\n",
    "            prob[sample_idx] = self.pi * (np.prod(np.power(self.beta, X[sample_idx]), axis=1))\n",
    "        return prob\n",
    "    \n",
    "    def predict_proba_with_log(self, X):\n",
    "        log_prob = self.predict_log_proba_with_log(X)\n",
    "        return np.exp(log_prob - np.max(log_prob, axis=1).reshape(-1, 1))\n",
    "    \n",
    "    def predict_log_proba_with_log(self, X):\n",
    "        log_prob = np.zeros((len(X), self.n_class))\n",
    "        '''\n",
    "            TODO: Calculate log-probability of which class each data belongs to, using self.log_beta and self.log_pi\n",
    "        '''\n",
    "        for sample_idx in range(len(X)):\n",
    "            # log_prob[sample_idx] = self.log_pi + np.sum(np.prod(self.log_beta, X[sample_idx]), axis=1)\n",
    "            # print(np.prod(self.log_beta, X[sample_idx]))\n",
    "            # print(self.log_beta.shape)\n",
    "            # print(X[sample_idx].shape)\n",
    "            # import time\n",
    "            # time.sleep(3600)\n",
    "            log_prob[sample_idx] = self.log_pi + np.sum(self.log_beta*X[sample_idx], axis=1)        \n",
    "        \n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Try your Multinomial Naive Bayes Implementation on a real-world dataset and compare with the model implemented in *sklearn*.\n",
    "   \n",
    "    We've already provided the data processing code below, which will help you download 20newsgroups dataset and extract word frequency feature for each document. We also provide the code for training and predict the probability of test data, using *sklearn* implementation.\n",
    "    \n",
    "    Now, try to train the model you implement, and calculate the probability of test data using ***predict_proba_without_log*** and ***predict_log_proba_with_log***. Compare the result with what we got by *sklearn* model, are they same or not? If they are different, please try to explain the reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian',  'comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "twenty_test  = fetch_20newsgroups(subset='test',  categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "count_vect = CountVectorizer().fit(twenty_train['data'] + twenty_test['data'])\n",
    "X_train_feature = count_vect.transform(twenty_train['data']).toarray()\n",
    "X_test_feature  = count_vect.transform(twenty_test['data']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_feature, twenty_train.target)\n",
    "pred_proba = clf.predict_proba(X_test_feature)\n",
    "pred = clf.predict(X_test_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n    TODO: Train your model, and then get the probability result using \"predict_proba_without_log\" and \"predict_proba_with_log\"\\n'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 124
    }
   ],
   "source": [
    "my_clf = My_MultinomialNB()\n",
    "'''\n",
    "    TODO: Train your model, and then get the probability result using \"predict_proba_without_log\" and \"predict_proba_with_log\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Is pred_proba_without_log from my_clf close to pred_proba from sklearn? False\n",
      "Is pred_proba_with_log from my_clf close to pred_proba from sklearn? False\n",
      "Test accuracy of sklearn model: 0.9347536617842876\n",
      "Test accuracy of our model (without log): 0.36684420772303594\n",
      "Test accuracy of our model (with log): 0.9347536617842876\n",
      "Does my_clf (with log) give the same prediction as sklearn? True\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "my_clf.fit(X_train_feature, twenty_train.target)\n",
    "pred_proba_without_log = my_clf.predict_proba_without_log(X_test_feature)\n",
    "pred_proba_with_log = my_clf.predict_proba_with_log(X_test_feature)\n",
    "print(\"Is pred_proba_without_log from my_clf close to pred_proba from sklearn? {}\".format(np.allclose(pred_proba, pred_proba_without_log)))\n",
    "print(\"Is pred_proba_with_log from my_clf close to pred_proba from sklearn? {}\".format(np.allclose(pred_proba, pred_proba_with_log)))\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Test accuracy of sklearn model: {}\".format(accuracy_score(twenty_test.target, pred)))\n",
    "print(\"Test accuracy of our model (without log): {}\".format(accuracy_score(twenty_test.target, my_clf.predict(X_test_feature, with_log=False))))\n",
    "my_pred_with_log = my_clf.predict(X_test_feature, with_log=True)\n",
    "print(\"Test accuracy of our model (with log): {}\".format(accuracy_score(twenty_test.target, my_pred_with_log)))\n",
    "print(\"Does my_clf (with log) give the same prediction as sklearn? {}\".format(np.array_equal(my_pred_with_log, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since sklearn performs normalization but we don't, there will be difference in the actual probability values; the predicted labels are identical, though. Also, the non-log version doesn't yield reasonable results because of overflowing issues."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Now, provide the evaluation result of your model, using accuracy as evaluation metric. \n",
    "    Next, choose different laplacian smoothing parameter ***alpha***, including (0, 0.001, 0.01, 0.1, 1, 10, 100, 1000). Plot the accuracy curve with different ***alpha*** using *matplotlib* package or *seaborn* package. \n",
    "    \n",
    "3. What is the best ***alpha***? Please explain why when ***alpha*** = 0 and ***alpha*** = 1000, the performance is relatively worse than the best case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Predicting when alpha is 0.\n",
      "Predicting when alpha is 0.001.\n",
      "Predicting when alpha is 0.1.\n",
      "Predicting when alpha is 1.\n",
      "Predicting when alpha is 10.\n",
      "Predicting when alpha is 100.\n",
      "Predicting when alpha is 1000.\n",
      "Predicting when alpha is 10000.\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "<ipython-input-80-72195c3d5677>:34: RuntimeWarning: divide by zero encountered in log\n",
      "  self.log_beta = np.log(self.beta)\n",
      "<ipython-input-80-72195c3d5677>:69: RuntimeWarning: invalid value encountered in multiply\n",
      "  log_prob[sample_idx] = self.log_pi + np.sum(self.log_beta*X[sample_idx], axis=1)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    '''\n",
    "        TODO: Implement accuracy metric\n",
    "    '''\n",
    "    assert len(y_pred) == len(y_true)\n",
    "    return ((y_pred == y_true).sum())/len(y_true)\n",
    "\n",
    "accs = []\n",
    "for alpha in [0, 0.001, 0.1, 1, 10, 100, 1000, 10000]:\n",
    "    '''\n",
    "        TODO: Train the model with different alpha, and get corresponding a\n",
    "    '''\n",
    "    print(\"Predicting when alpha is {}.\".format(alpha))\n",
    "    clf = My_MultinomialNB(alpha=alpha)\n",
    "    clf.fit(X_train_feature, twenty_train.target)\n",
    "    pred = clf.predict(X_test_feature, with_log=True)\n",
    "    a = accuracy(twenty_test.target, pred)\n",
    "    accs.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(accs)\n",
    "plt.xticks(np.arange(8), [0, 0.001, 0.01, 0.1, 1, 10, 100, 1000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the plot above, $$\\alpha$$ being 0.001 gives the best accuracy.\n",
    "<br><br>\n",
    "$$\\alpha$$ being 0 performs really bad because it means that there's no smoothing. $$\\beta_{jn}$$ will degrade to 0 often.\n",
    "<br><br>\n",
    "$$\\alpha$$ being 10000 also performs really bad because since $$\\beta_{jn}=\\frac{\\mathrm{something}+\\alpha}{\\mathrm{somethingelse}+\\alpha N}$$. With $$\\alpha\\rightarrow\\infty, \\beta\\rightarrow\\frac{1}{N}$$ which is just a constant."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}