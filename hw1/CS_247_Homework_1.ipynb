{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 for CS 247 : Advanced Data Mining Learning\n",
    "\n",
    "### Due: 11:59 pm 04/07\n",
    "\n",
    "\n",
    "__Name__: [Your name]\n",
    "\n",
    "__UID__: [Your uid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 Multinomial Naive Bayes Optimization\n",
    "\n",
    "For multinomial naive Bayes model, prove the MLE estimator Î² for what is stated in Slide 21."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Your answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2:  Multinomial Naive Bayes Implementation\n",
    "\n",
    "\n",
    "In this problem, we'd like you to implement naive bayes, and apply it on a real-world sentiment classification dataset. \n",
    "\n",
    "1. We've provided a general framework \"My_MultinomialNB\" below, please fill ''TODO'' slots. More specifically, you should implement the ***fit***, ***predict_proba_without_log*** and ***predict_log_proba_with_log*** functions. \n",
    "\n",
    "    For function ***fit***, given a training dataset with feature ***X*** and label ***y***, we'd like you to calculate ***beta*** and ***pi*** with a smoothing parameter ***alpha*** (laplace smoothing). \n",
    "\n",
    "    For ***predict_proba_without_log***, given a test dataset with feature X, we'd like you to calculate the predicted probability of each data point using what we've learned in class.\n",
    "\n",
    "    For ***predict_log_proba_with_log***, given a test dataset with feature ***X***, we'd like you to calculate the log probability of each data point, using ***log_beta*** and ***log_pi***. With this function, we can also get probability using ***predict_proba_with_log***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_MultinomialNB():\n",
    "    \"\"\"\n",
    "    Multinomial Naive Bayes (MultinomialNB)\n",
    "    ==========  \n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : float, optional (default=1.0)\n",
    "        Additive (Laplace/Lidstone) smoothing parameter\n",
    "        (0 for no smoothing).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=1):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.class_indicator = {}\n",
    "        for i, c in enumerate(np.unique(y)):\n",
    "            self.class_indicator[c] = i\n",
    "        self.n_class = len(self.class_indicator)\n",
    "        self.n_feats = np.shape(X)[1]\n",
    "        \n",
    "        self.beta    = np.zeros((self.n_class, self.n_feats))\n",
    "        self.pi      = np.zeros((self.n_class))\n",
    "        '''\n",
    "            TODO: Calculate self.beta and self.pi\n",
    "        '''\n",
    "        self.log_beta = np.log(self.beta)\n",
    "        self.log_pi   = np.log(self.pi)\n",
    "        \n",
    "    def predict(self, X, with_log = True):\n",
    "        if with_log:\n",
    "            probability = self.predict_proba_with_log(X)\n",
    "        else:\n",
    "            probability = self.predict_proba_without_log(X)\n",
    "        return np.argmax(probability, axis=1)    \n",
    "    \n",
    "    def predict_proba_without_log(self, X):\n",
    "        prob = np.zeros((len(X), self.n_class))\n",
    "        '''\n",
    "            TODO: Calculate probability of which class each data belongs to, using self.beta and self.pi\n",
    "        '''\n",
    "        return prob\n",
    "    \n",
    "    def predict_proba_with_log(self, X):\n",
    "        log_prob = self.predict_log_proba_with_log(X)\n",
    "        return np.exp(log_prob - np.max(log_prob, axis=1).reshape(-1, 1))\n",
    "    \n",
    "    def predict_log_proba_with_log(self, X):\n",
    "        log_prob = np.zeros((len(X), self.n_class))\n",
    "        '''\n",
    "            TODO: Calculate log-probability of which class each data belongs to, using self.log_beta and self.log_pi\n",
    "        '''\n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Try your Multinomial Naive Bayes Implementation on a real-world dataset and compare with the model implemented in *sklearn*.\n",
    "   \n",
    "    We've already provided the data processing code below, which will help you download 20newsgroups dataset and extract word frequency feature for each document. We also provide the code for training and predict the probability of test data, using *sklearn* implementation.\n",
    "    \n",
    "    Now, try to train the model you implement, and calculate the probability of test data using ***predict_proba_without_log*** and ***predict_log_proba_with_log***. Compare the result with what we got by *sklearn* model, are they same or not? If they are different, please try to explain the reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian',  'comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "twenty_test  = fetch_20newsgroups(subset='test',  categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "count_vect = CountVectorizer().fit(twenty_train['data'] + twenty_test['data'])\n",
    "X_train_feature = count_vect.transform(twenty_train['data']).toarray()\n",
    "X_test_feature  = count_vect.transform(twenty_test['data']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_feature, twenty_train.target)\n",
    "pred_proba = clf.predict_proba(X_test_feature)\n",
    "pred = clf.predict(X_test_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf = My_MultinomialNB()\n",
    "'''\n",
    "    TODO: Train your model, and then get the probability result using \"predict_proba_without_log\" and \"predict_proba_with_log\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Now, provide the evaluation result of your model, using accuracy as evaluation metric. \n",
    "    Next, choose different laplacian smoothing parameter ***alpha***, including (0, 0.001, 0.01, 0.1, 1, 10, 100, 1000). Plot the accuracy curve with different ***alpha*** using *matplotlib* package or *seaborn* package. \n",
    "    \n",
    "3. What is the best ***alpha***? Please explain why when ***alpha*** = 0 and ***alpha*** = 1000, the performance is relatively worse than the best case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    '''\n",
    "        TODO: Implement accuracy metric\n",
    "    '''\n",
    "    return 0\n",
    "\n",
    "accs = []\n",
    "for alpha in [0, 0.001, 0.1, 1, 10, 100, 1000, 10000]:\n",
    "    '''\n",
    "        TODO: Train the model with different alpha, and get corresponding a\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(accs)\n",
    "plt.xticks(np.arange(8), [0, 0.001, 0.01, 0.1, 1, 10, 100, 1000])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}